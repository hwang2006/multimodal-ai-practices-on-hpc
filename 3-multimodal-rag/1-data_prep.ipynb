{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "193face1-334a-43a0-874b-b2f8048ca963",
   "metadata": {},
   "source": [
    "# Data Prep\n",
    "\n",
    "Code authored by: Shaw Talebi \n",
    "\n",
    "[Blog link](https://medium.com/towards-data-science/multimodal-rag-process-any-file-type-with-ai-e6921342c903) \n",
    "| [Video link](https://youtu.be/Y7pNmocrmi8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3b98ec-bbd6-4f28-afaf-5a30616a51f0",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6901a467-4d59-4dbe-9e15-47bbec842b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from functions import *\n",
    "\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "from torch import cat, save"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fb3755-817c-484f-8227-df968b5ad909",
   "metadata": {},
   "source": [
    "### Extract text and images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7c9f1f5-65cc-49fe-97e0-79e5ed827388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['raw/2024-11-19_Multimodal-Models---LLMs-that-can-see-and-hear-5c6737c981d3.html', 'raw/2024-11-29_Multimodal-Embeddings--An-Introduction-5dc36975966f.html']\n"
     ]
    }
   ],
   "source": [
    "# Extract text and images\n",
    "\n",
    "root_dir = \"raw\"\n",
    "\n",
    "# raw ë””ë ‰í„°ë¦¬ ì•ˆì—ì„œ \"íŒŒì¼\"ì´ë©´ì„œ .html/.htm ì¸ ê²ƒë§Œ ì„ íƒ\n",
    "filename_list = []\n",
    "for name in os.listdir(root_dir):\n",
    "    path = os.path.join(root_dir, name)\n",
    "    if os.path.isfile(path) and path.lower().endswith((\".html\", \".htm\")):\n",
    "        filename_list.append(path)\n",
    "\n",
    "print(filename_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee948ec1-cbe1-453b-b4e2-9bdbc149795b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all HTML files from raw directory\n",
    "#filename_list = [\"raw/\"+f for f in os.listdir('raw')]\n",
    "\n",
    "text_content_list = []\n",
    "image_content_list = []\n",
    "for filename in filename_list:\n",
    "\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        html_content = file.read()\n",
    "\n",
    "    text_content_list.extend(parse_html_content(html_content))\n",
    "    image_content_list.extend(parse_html_images(html_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccaf6195-7026-4348-b05a-102be2ebbb8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86\n",
      "17\n"
     ]
    }
   ],
   "source": [
    "print(len(text_content_list))\n",
    "print(len(image_content_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d771e2-9037-4f82-9ff9-1e764d2e8d0f",
   "metadata": {},
   "source": [
    "### You have 17 entries in image_content_list, but if you look closely:\n",
    "\n",
    "13 https://cdn-images-1.medium.com/max/800/1*4wnqr5p_7N3QD5EkXIQeew.png\n",
    "\n",
    "15 https://cdn-images-1.medium.com/max/800/1*4wnqr5p_7N3QD5EkXIQeew.png  (duplicate)\n",
    "\n",
    "12 https://cdn-images-1.medium.com/max/800/1*Nzo536sqahqm1Q24Ms2vmA.png\n",
    "\n",
    "16 https://cdn-images-1.medium.com/max/800/1*Nzo536sqahqm1Q24Ms2vmA.png   (duplicate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "190e69e4-769c-4615-a3a6-5919c2e309d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 https://cdn-images-1.medium.com/max/800/0*YE-Q-OuWnrgrUrQw | Photo by Sincerely Media onÂ Unsplash\n",
      "1 https://cdn-images-1.medium.com/max/800/1*yvfu8VAp1UgCw4SVvUe77Q.png | Example mutlimodal models. Image byÂ author.\n",
      "2 https://cdn-images-1.medium.com/max/800/1*Nwc-ZhRFKH17LWWmsNhbdA.png | An example of information degradation during message passing. Image byÂ author.\n",
      "3 https://cdn-images-1.medium.com/max/800/1*pyqGh5Cbrk_EMlPYtrfrQw.png | A simple strategy for integrating images into an LLM via an image encoding adapter. Image byÂ author.\n",
      "4 https://cdn-images-1.medium.com/max/800/1*lvX8Mut8SQ1vDhsaewLQ_g.jpeg | Image of me from networking event at Richardson IQ. Image byÂ author.\n",
      "5 https://cdn-images-1.medium.com/max/800/1*IqUoZEX2CYOsX6oFIVeuIw.jpeg | Building with AI meme. Image byÂ author.\n",
      "6 https://cdn-images-1.medium.com/max/800/1*PRSGngwjIVW01cLHK41lNg.jpeg | Screenshot of 5 AI project ideas. Image byÂ author.\n",
      "7 https://cdn-images-1.medium.com/max/800/1*a6BF-kEeo8rd7OW2a3JYGA.png | Image fromÂ Canva.\n",
      "8 https://cdn-images-1.medium.com/max/800/1*jpmC6Kx7DxVeikEr15vooA.png | Toy represetation of text and image embeddings, respectively. Image byÂ author.\n",
      "9 https://cdn-images-1.medium.com/max/800/1*5d3HBNjNIXLy0oMIvJjxWw.png | Toy representation of multimodal embedding space. Image byÂ author.\n",
      "10 https://cdn-images-1.medium.com/max/800/1*AGHBVjzwjXapJSe4aUPrjg.png | Example positive and negative pairs used in contrastive training. Image byÂ author.\n",
      "11 https://cdn-images-1.medium.com/max/800/1*2X1aT8fzFsgbqn23zXmmAA.png | CLIPâ€™s contrastive loss for text-image representation alignment [3]. Image byÂ author.\n",
      "12 https://cdn-images-1.medium.com/max/800/1*Nzo536sqahqm1Q24Ms2vmA.png | Input cat photo. Image fromÂ Canva.\n",
      "13 https://cdn-images-1.medium.com/max/800/1*4wnqr5p_7N3QD5EkXIQeew.png | Best match for query â€œa cute dogâ€. Image fromÂ Canva.\n",
      "14 https://cdn-images-1.medium.com/max/800/1*tIY3_ONQQT_cracAPWm8NQ.png | Best match for query â€œsomething cute but metal ðŸ¤˜â€. Image fromÂ Canva.\n",
      "15 https://cdn-images-1.medium.com/max/800/1*4wnqr5p_7N3QD5EkXIQeew.png | Best match for query â€œa good boyâ€. Image fromÂ Canva.\n",
      "16 https://cdn-images-1.medium.com/max/800/1*Nzo536sqahqm1Q24Ms2vmA.png | Best match for query â€œthe best pet in the worldâ€. Image fromÂ Canva.\n"
     ]
    }
   ],
   "source": [
    "for i, content in enumerate(image_content_list):\n",
    "    print(i, content['image_path'], \"|\", content['caption'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb165b1-4749-40c9-881e-47d7539cffda",
   "metadata": {},
   "source": [
    "## On an HPC system, Compute / login nodes often have no outbound internet or very restricted HTTP(S)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c2d369-a0c4-41af-ab0b-d4d75d0dab8c",
   "metadata": {},
   "source": [
    "### Step 1: Collect local images sorted numerically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17a35ecc-712c-4e42-893e-2b6b1d92e92b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local image paths (numeric sorted):\n",
      "images/1.jpg\n",
      "images/2.png\n",
      "images/3.png\n",
      "images/4.png\n",
      "images/5.png\n",
      "images/6.png\n",
      "images/7.jpeg\n",
      "images/8.png\n",
      "images/9.jpeg\n",
      "images/10.png\n",
      "images/11.png\n",
      "images/12.jpeg\n",
      "images/13.png\n",
      "images/14.png\n",
      "images/15.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "image_dir = \"images\"\n",
    "\n",
    "local_files = []\n",
    "for fname in os.listdir(image_dir):\n",
    "    name, ext = os.path.splitext(fname)\n",
    "    if ext.lower() in (\".png\", \".jpg\", \".jpeg\"):\n",
    "        try:\n",
    "            idx = int(name)  # \"1\" -> 1\n",
    "        except ValueError:\n",
    "            continue  # skip non-numeric names\n",
    "        \n",
    "        local_files.append((idx, os.path.join(image_dir, fname)))\n",
    "\n",
    "# sort by numeric index: 1,2,...,15\n",
    "local_files.sort(key=lambda x: x[0])\n",
    "local_paths = [path for _, path in local_files]\n",
    "\n",
    "print(\"Local image paths (numeric sorted):\")\n",
    "for p in local_paths:\n",
    "    print(p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363a7a7f-85d1-470d-90a0-26c2782055d0",
   "metadata": {},
   "source": [
    "### Step 2: Build the list of unique image URLs (in order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4859de43-efde-4ac6-9839-b0c74b32368f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique URLs: 15\n",
      "0 https://cdn-images-1.medium.com/max/800/0*YE-Q-OuWnrgrUrQw\n",
      "1 https://cdn-images-1.medium.com/max/800/1*yvfu8VAp1UgCw4SVvUe77Q.png\n",
      "2 https://cdn-images-1.medium.com/max/800/1*Nwc-ZhRFKH17LWWmsNhbdA.png\n",
      "3 https://cdn-images-1.medium.com/max/800/1*pyqGh5Cbrk_EMlPYtrfrQw.png\n",
      "4 https://cdn-images-1.medium.com/max/800/1*lvX8Mut8SQ1vDhsaewLQ_g.jpeg\n",
      "5 https://cdn-images-1.medium.com/max/800/1*IqUoZEX2CYOsX6oFIVeuIw.jpeg\n",
      "6 https://cdn-images-1.medium.com/max/800/1*PRSGngwjIVW01cLHK41lNg.jpeg\n",
      "7 https://cdn-images-1.medium.com/max/800/1*a6BF-kEeo8rd7OW2a3JYGA.png\n",
      "8 https://cdn-images-1.medium.com/max/800/1*jpmC6Kx7DxVeikEr15vooA.png\n",
      "9 https://cdn-images-1.medium.com/max/800/1*5d3HBNjNIXLy0oMIvJjxWw.png\n",
      "10 https://cdn-images-1.medium.com/max/800/1*AGHBVjzwjXapJSe4aUPrjg.png\n",
      "11 https://cdn-images-1.medium.com/max/800/1*2X1aT8fzFsgbqn23zXmmAA.png\n",
      "12 https://cdn-images-1.medium.com/max/800/1*Nzo536sqahqm1Q24Ms2vmA.png\n",
      "13 https://cdn-images-1.medium.com/max/800/1*4wnqr5p_7N3QD5EkXIQeew.png\n",
      "14 https://cdn-images-1.medium.com/max/800/1*tIY3_ONQQT_cracAPWm8NQ.png\n"
     ]
    }
   ],
   "source": [
    "unique_urls = []\n",
    "for item in image_content_list:\n",
    "    url = item[\"image_path\"]\n",
    "    if url not in unique_urls:\n",
    "        unique_urls.append(url)\n",
    "\n",
    "print(\"Unique URLs:\", len(unique_urls))\n",
    "for i, url in enumerate(unique_urls):\n",
    "    print(i, url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1649355-cb63-428c-a016-85536150d333",
   "metadata": {},
   "source": [
    "### Step 3: Create a URL â†’ local file mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56d101fc-ec68-4417-aecf-7669b32eb092",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(unique_urls) == len(local_paths), \"Mismatch between unique URLs and local images\"\n",
    "\n",
    "url_to_path = {url: local_paths[i] for i, url in enumerate(unique_urls)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c56f29-5d4e-4ca8-a22d-c5c8cdeec615",
   "metadata": {},
   "source": [
    "#### If you want to be extra safe, you can also store the original URL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52ac0c4c-efff-492d-a860-475d8e404893",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in image_content_list:\n",
    "    url = item[\"image_path\"]\n",
    "    item[\"source_url\"] = url  # keep original URL\n",
    "    if url in url_to_path:\n",
    "        item[\"image_path\"] = url_to_path[url]\n",
    "    else:\n",
    "        print(\"WARNING: no local file mapped for\", url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2a28e8-e551-43d1-9c77-29cc90043209",
   "metadata": {},
   "source": [
    "### Step 4: Build image_list safely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1413d871-e9f4-4a8a-ba90-79888f7733f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "image_list = []\n",
    "for content in image_content_list:\n",
    "    try:\n",
    "        img = Image.open(content[\"image_path\"]).convert(\"RGB\")\n",
    "        image_list.append(img)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Missing local file:\", content[\"image_path\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778f453d-8b0d-4745-8600-82bd773f9e72",
   "metadata": {},
   "source": [
    "### Build test_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3b6eff4-fa47-4a35-822e-bc125c197f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = []\n",
    "for content in text_content_list:\n",
    "    # concatenate title and section header\n",
    "    section = content['section'] + \": \"\n",
    "    # append text from paragraph to fill CLIP's 256 sequence limit\n",
    "    text = section + content['text'][:256-len(section)]\n",
    "    \n",
    "    text_list.append(text)\n",
    "\n",
    "#image_list = []\n",
    "#for content in image_content_list:\n",
    "#    image_list.append(Image.open(content['image_path']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48ecd598-3fb0-4fc2-a0a7-be334bb3f89e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86\n",
      "17\n"
     ]
    }
   ],
   "source": [
    "print(len(text_list))\n",
    "print(len(image_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f8710b-63a9-464a-b13e-f19be08669ea",
   "metadata": {},
   "source": [
    "### Compute embeddings using CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "edcc16c0-0ef1-4e21-bff3-39b6af546fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "# import model\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "\n",
    "# import processor (handles text tokenization and image preprocessing)\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch16\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5ff2d5f-9d01-42a6-b1a1-ebe9c6a594d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-process text and images\n",
    "inputs = processor(text=text_list, images=image_list, return_tensors=\"pt\", padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "17e25fb1-83d3-4a45-9d0f-0cde281228cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute embeddings with CLIP\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "147b2dff-8180-449e-abfe-22fe492c7636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store embeddings in single torch tensor\n",
    "text_embeddings = outputs.text_embeds\n",
    "image_embeddings = outputs.image_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee24d96b-c0a8-46c4-837a-bbab69a96a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([86, 512])\n",
      "torch.Size([17, 512])\n"
     ]
    }
   ],
   "source": [
    "print(text_embeddings.shape)\n",
    "print(image_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66db9d6e-4c39-4180-89c7-d8108bee7c9a",
   "metadata": {},
   "source": [
    "### Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "39af8e14-1909-490e-ba6b-d32170d12711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save content list as JSON\n",
    "save_to_json(text_content_list, output_file='data/text_content.json')\n",
    "save_to_json(image_content_list, output_file='data/image_content.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "db55fd6e-921c-49a0-9c9c-83d0ac80b88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save embeddings to file\n",
    "save(text_embeddings, 'data/text_embeddings.pt')\n",
    "save(image_embeddings, 'data/image_embeddings.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
