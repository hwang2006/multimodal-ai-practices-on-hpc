{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "383ce849-febb-4663-94a4-462f4ce78c8b",
   "metadata": {},
   "source": [
    "# Chat UI for Multimodal Article QA Assistant\n",
    "\n",
    "Code authored by: Shaw Talebi\n",
    "\n",
    "[Blog link](https://medium.com/towards-data-science/multimodal-rag-process-any-file-type-with-ai-e6921342c903) \n",
    "| [Video link](https://youtu.be/Y7pNmocrmi8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c723c318-0791-4da3-a95c-d4eb9ba3d9a5",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "673ddbfa-9d69-4148-a31c-84b78653b553",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1c661af-fd74-424c-a56c-9b36bd0225d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "from torch import load\n",
    "import gradio as gr\n",
    "import time\n",
    "from functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf4a0c2-a727-4c84-8ccb-470490653e6f",
   "metadata": {},
   "source": [
    "### Load model and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af411ded-fecb-4d6e-8c2f-71efaea90f5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProgressResponse(status='success', completed=None, total=None, digest=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pull model\n",
    "ollama.pull('llama3.2-vision')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59cd245e-0983-4719-96a9-e0a7ad420c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load article contents\n",
    "text_content_list = load_from_json('data/text_content.json')\n",
    "image_content_list = load_from_json('data/image_content.json')\n",
    "\n",
    "# load embeddings\n",
    "text_embeddings = load('data/text_embeddings.pt', weights_only=True)\n",
    "image_embeddings = load('data/image_embeddings.pt', weights_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e099fadd-de60-47d5-975e-063c119acf8e",
   "metadata": {},
   "source": [
    "### UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d5249c6-5c31-443d-8f85-44ba97d4d5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to interact with the Ollama model\n",
    "def stream_chat(message, history):\n",
    "    \"\"\"\n",
    "    Streams the response from the Ollama model and sends it to the Gradio UI.\n",
    "    \n",
    "    Args:\n",
    "        message (str): The user input message.\n",
    "        history (list): A list of previous conversation messages.\n",
    "        \n",
    "    Yields:\n",
    "        str: The chatbot's response chunk by chunk.\n",
    "    \"\"\"\n",
    "\n",
    "    # context retrieval\n",
    "    text_results, image_results = context_retrieval(message[\"text\"], text_embeddings, image_embeddings, text_content_list, image_content_list)\n",
    "\n",
    "    # construct prompt\n",
    "    prompt = construct_prompt(message[\"text\"], text_results, image_results)\n",
    "    \n",
    "    # Append the user message to the conversation history\n",
    "    history.append({\"role\": \"user\", \"content\": prompt, \"images\": [image[\"image_path\"] for image in image_results]})\n",
    "    \n",
    "    # Initialize streaming from Ollama\n",
    "    stream = ollama.chat(\n",
    "        model='llama3.2-vision',\n",
    "        messages=history,  # Full chat history including the current user message\n",
    "        stream=True,\n",
    "    )\n",
    "    \n",
    "    response_text = \"\"\n",
    "    for chunk in stream:\n",
    "        content = chunk['message']['content']\n",
    "        response_text += content\n",
    "        yield response_text  # Send the response incrementally to the UI\n",
    "\n",
    "    # Append the assistant's full response to the history\n",
    "    history.append({\"role\": \"assistant\", \"content\": response_text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aeede1f9-b660-4e96-885e-3350d8cc800c",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ChatInterface.__init__() got an unexpected keyword argument 'type'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Create a Gradio ChatInterface\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mgr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mChatInterface\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_chat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# The function handling the chat\u001b[39;49;00m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Using \"messages\" to enable chat-style conversation\u001b[39;49;00m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mWhat is CLIP\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43ms contrastive loss function?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m              \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mWhat are the three paths described for making LLMs multimodal?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m              \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mWhat is an intuitive explanation of multimodal embeddings?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Example inputs\u001b[39;49;00m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmultimodal\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m)\u001b[49m.launch()\n",
      "\u001b[31mTypeError\u001b[39m: ChatInterface.__init__() got an unexpected keyword argument 'type'"
     ]
    }
   ],
   "source": [
    "# Create a Gradio ChatInterface\n",
    "gr.ChatInterface(\n",
    "    fn=stream_chat,  # The function handling the chat\n",
    "    type=\"messages\",  # Using \"messages\" to enable chat-style conversation\n",
    "    examples=[{\"text\": \"What is CLIP's contrastive loss function?\"}, \n",
    "              {\"text\": \"What are the three paths described for making LLMs multimodal?\"},\n",
    "              {\"text\": \"What is an intuitive explanation of multimodal embeddings?\"}],  # Example inputs\n",
    "    multimodal=True,\n",
    ").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1816024e-a1b7-427b-9479-d7874ffd6c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_chat(message, history):\n",
    "    # message is now a plain string\n",
    "    query = message\n",
    "\n",
    "    # context retrieval\n",
    "    text_results, image_results = context_retrieval(\n",
    "        query, text_embeddings, image_embeddings, text_content_list, image_content_list\n",
    "    )\n",
    "\n",
    "    # construct prompt\n",
    "    prompt = construct_prompt(query, text_results, image_results)\n",
    "\n",
    "    # Append the user message (with images) to history for Ollama\n",
    "    history.append({\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt,\n",
    "        \"images\": [image[\"image_path\"] for image in image_results],\n",
    "    })\n",
    "\n",
    "    stream = ollama.chat(\n",
    "        model='llama3.2-vision',\n",
    "        messages=history,\n",
    "        stream=True,\n",
    "    )\n",
    "\n",
    "    response_text = \"\"\n",
    "    for chunk in stream:\n",
    "        content = chunk[\"message\"][\"content\"]\n",
    "        response_text += content\n",
    "        yield response_text\n",
    "\n",
    "    history.append({\"role\": \"assistant\", \"content\": response_text})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76e75a16-7514-4b5e-8aa6-2cca850dece7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "gr.ChatInterface(\n",
    "    fn=stream_chat,  # your streaming function is already (message, history)\n",
    "    examples=[\n",
    "        \"What is CLIP's contrastive loss function?\",\n",
    "        \"What are the three paths described for making LLMs multimodal?\",\n",
    "        \"What is an intuitive explanation of multimodal embeddings?\",\n",
    "    ],\n",
    ").launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7060e5dc-89de-4a0c-9c53-31a58e894c98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
